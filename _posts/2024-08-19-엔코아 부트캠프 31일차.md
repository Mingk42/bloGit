## json data flatten

오늘은 api를 통해 json형식으로 받은 data를 flatten하는 방법을 알아보았다.

#### explode

지난 광복절 및 주말 과제로 영화진흥위원회 api를 통해 받을 수 있는 모든 데이터를 받아보았다. api는 2개씩 짝지어질 수 있었는데 영화정보-영화상세정보, 영화사정보-영화사상세정보 와 같은 방식이다. 그리고 영화정보에서 얻은 영화코드를 api 호출시에 query로 사용하여 영화상세정보를 받을 수 있었다. 그런데 여기서 불편한 점이 한 가지 발생하였다. json으로 받은 데이터의 경우 우리가 목적으로 하는 데이터를 얻기까지 여러 번의 key를 거쳐야 했다. 즉, 중첩된 json형식이라서 원하는 데이터에 바로 접근하기 어렵다는 이야기다. XML의 경우 각 tag가 유일해 parsing만 하면 바로 원하는 값에 접근할 수 있는 것과 대조되어 더욱 불편하게 느껴졌다. (물론, parsing의 과정을 거치지 않고 key를 이용하여 값을 얻을 수 있다는 것은 json의 장점이다.)

이러한 불편을 해결하는 방법이 flatten이다. 단어 그 자체의 뜻은 납작하게 만든다는 뜻인데, 데이터를 펼쳐서 평평하게 만드는 것을 의미한다. 말이 어려우니 데이터를 확인해보자.

```python
%spark.pyspark
> df = spark.read.option("multiline","true").json('/home/root2/data/movies/movieList')
> df.printSchema()
root
 |-- companys: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- companyCd: string (nullable = true)
 |    |    |-- companyNm: string (nullable = true)
...
```
위 데이터에서 companyCd 값을 얻기 위해서는 `df["companys"][list idx]["companyCd"]`라는 단계를 거쳐야 한다. 이는 꽤 번거로운 과정이다. 그런데 이를 평탄화(flatten)하면 그 값에 조금 더 쉽게 접근할 수 있다. 평탄화는 아래와 같이 할 수 있다.

```python
%spark.pyspark
from pyspark.sql.functions import explode, explode_outer
> df1 = df.withColumn("company", explode("companys"))       # NULL 값 row 삭제
> df2 = df.withColumn("company", explode_outer("companys")) # NULL 값 row 유지
> df2.printSchema()
root
 |-- company: struct (nullable = true)
 |    |-- companyCd: string (nullable = true)
 |    |-- companyNm: string (nullable = true)
...
```
`explode`라는 함수만 사용하면 바로 평탄화가 이뤄진다. 그리고 이렇게 평단화가 이뤄진 데이터는 company.companyCd라는 훨씬 간단한 방법으로 사용할 수 있게 된다! 게다가 withColumn함수를 통해 company.companyCd 자체를 하나의 column으로 재할당하면 마치 중첩된 column이 아닌 것처럼 사용할 수 있게 된다! 


### 정리

flatten을 통해 json데이터를 훨씬 편리하게 사용할 수 있는 방법을 알게 되어 의미있는 하루였던 것 같다.
