## spark on airflow

오늘은 한 발짝 나가서 airflow에서 spark를 사용하는 방법을 알아보았다.

#### shell script로써 spark 사용해보기

```python
"""SimpleApp.py"""
from pyspark.sql import SparkSession

logFile = "/home/root2/app/spark-3.5.1-bin-hadoop3/README.md"  # Should be some file on your system
spark = SparkSession.builder.appName("SimpleApp").getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print("*"*1000)
print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

spark.stop()
```
위 코드는 spark docs에 있는 간단한 spark프로그램이다. 이를 아래와 같은 방법으로 사용할 수 있다.

```bash
$ $SPARK_HOME/bin/spark-submit SimpleApp.py
...
Lines with a: 72, lines with b: 39
...
```
수 많은 log가 찍히는데 중간에 위와 같은 결과값이 출력되었다. 이를 쉽게 찾기 위해 `print("*"*1000)`을 추가해주었다. console에서 spark프로그램을 동작시킬 수 있다는 것을 알게 되었으니 다음은 간단하다! spark프로그램을 만들고 BashOperator로 실행하도록 만들면 되는 것이다!!

그 결과물은 [여기로!](https://github.com/Mingk42/mingk42_airflow_pyspark)

### 정리

간단하게 적었지만 전체적인 로직을 작성하는데 시간이 조금 걸렸다. 중간에 spark에서 parquet으로 저장하는 부분에서 오류가 발생했다. 처음에는 mode를 지정하지 않았었는데 이미 존재하는 path라며 오류가 발생했고 mode를 overwrite로 변경하라는 문구가 나왔다. 그런데 overwrite로 mode를 지정하면 정말 치명적이게도! 기존에 존재하는 parquet를 지워버리고 새로운 parquet를 생성했다. 문제는 이게 같은 경로로 지정되는 경우가 아니더라도 (이해가 어려울까봐 다시 말하자면 load_dt를 기준으로 partition한다고 할 때, load_dt가 다른 경우에도!) 있던 것을 지워버리고 덮어쓴다는 것이다. 이 문제를 해결하기 위해 append를 사용하긴 했는데, 아마 멱등성에 문제가 생길 수 있을 것 같다(다시 실행할 경우 계속해서 새로운 parquet가 추가될 것이므로). 조금 고민이 필요할 것 같다. 
