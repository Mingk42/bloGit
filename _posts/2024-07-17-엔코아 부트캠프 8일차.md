## airflow 맛보기
회사가 친목을 쌓기 위한 곳은 아니지만, 일은 잘하지만 협업이 안되는 사람보다 일은 조금 못하더라도 팀으로서 협업을 잘할 수 있는 사람이 더 같이 일하고 싶은 사람이라는 말씀을 해주셨다.

오늘도 강조되는 협업의 중요성! 

협업! 화이팅!

#### airflow
오늘은 airflow를 조금만 알아보기로 했다.

airflow는 airbnb에서 만든 것이라고 한다. 이용자 규모가 커지면서 이런 프로그램이 자연스럽게 요구됐을 것 같다는 생각이 들>었다.

먼저 설치. [공식문서](https://airflow.apache.org/docs/apache-airflow/stable/start.html)에 따라 진행하면 된다.

1번 단계는 선택적이라고 되어있는 부분이긴 한데 .zshrc에 환경변수를 적어주었다.
```bash
export AIRFLOW_HOME=~/airflow
```

그리고 나서 실제 설치단계를 진행하였다.
```bash
$ AIRFLOW_VERSION=2.9.3
$ PYTHON_VERSION="$(python -c 'import sys; print(f"{sys.version_info.major}.{sys.version_info.minor}")')"
$ CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

$ pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

그리고 나서 `airflow standalone`을 입력하면 개발환경이 구동된다.
```bash
$ airflow standalone
...
webserver  | [2024-07-17 12:15:40 +0900] [56925] [INFO] Listening at: http://0.0.0.0:8080 (56925)
...
standalone | Airflow is ready
standalone | Login with username: admin  password: ~~~~~~~
standalone | Airflow Standalone is for development purposes only. Do not use this in production!
```
`localhost:8080`으로 접속해보면 UI를 확인할 수 있다.

해당 페이지를 접속하려면 계정이 필요한데 메시지에서 나타나는 admin / password를 입력하면 접속할 수 있다.

그리고 마지막 메시지에 나타나지만 해당 환경은 개발용이라서 실제 운영용으로 사용하면 안된다!

#### dag
데이터 파이프라인을 테스트로 만들어보았다. 순서는 먼저 dags 디렉토리를 생성하고 .py파일을 만들면 된다.
```bash
$ cd $AIRFLOW_HOME            # ~/airflow, 최초 설치시 환경변수로 설정해두었음
$ mkdir dags
$ vi dags/helloworld.py       # dags 디렉토리 밑에 .py로 파일을 작성하면 airflow UI페이지에서 자동으로 load된다.
```
##### helloworld.py
```python
from datetime import datetime, timedelta
from textwrap import dedent

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator

with DAG(
    'helloworld',
    # These args will get passed on to each operator
    # You can override them on a per-task basis during operator initialization
    default_args={
        'depends_on_past': True,
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5)
    },
    description='hello world DAG',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2024, 7, 10),
    catchup=True,
    tags=['helloworld'],
) as dag:

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
    )

    t2 = BashOperator(
        task_id='sleep',
        depends_on_past=False,
        bash_command='sleep 5',
        retries=3,
    )

    t3 = DummyOperator(task_id='t3')
    t22 = DummyOperator(task_id='t22')
    t33 = DummyOperator(task_id='t33')
    task_end = DummyOperator(task_id='end')
    task_start = DummyOperator(task_id='start')
    task_empty = DummyOperator(task_id='empty')

    t1 >> [t2, t3] >> task_end                   # []안에 같이 작성하면 같은 순서에 위치한 프로세스가 된다.
    task_start >> t1                             # 작성순서와 무관하게 파이프라인을 만들 수 있다.
    t3 >> task_empty
    t1 >> [t22, t33] >> task_end
    task_empty >> task_end
```
그리고 나서 airflow UI페이지에서 보면 방금 만든 파이프라인이 조회된다. (파싱할 시간이 필요해서 바로 보이지 않을 수 있다.)

조회된 파이프라인의 구성도는 아래와 같다. (활성화시키고 파이프라인이 실행되는 도중의 이미지이다.)

![pl1](https://github.com/user-attachments/assets/2a65e80c-8148-4b7c-8731-8ca5f2b8393d)



### 정리

오늘은 airflow를 이용하여 데이터 파이프라인을 구성하는 방법을 배웠다. 데이터 파이프라인이라고 하면 뭔가 감이 잘 안잡히고 멀게 느껴졌는데, 이렇게 보니 생각보다 **지금까지는** 간단한 것 같다. 간단한 것 같으니 이해도 잘 되고 재미있는 것 같다.
                         
