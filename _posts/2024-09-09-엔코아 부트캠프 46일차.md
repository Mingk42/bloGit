## KNeighborsRegression, LinearRegression

#### KNeighborsRegression

지난 주의 분류에 이어서 오늘은 회귀모형에 대해 학습하였다. 먼저 KNeighborsRegression으로 회귀모형에 대한 학습을 시작하였다. KNeighborsRegression은 지난 주에 분류모형에 사용했던 KNN과 비슷한 방식으로 동작하는데 근접한 이웃들의 평균울 회귀모형의 추정값으로 반환하는 방식이라고 한다. 그리고 이러한 모형의 한계는 뚜렷하다.

```
>>> knr=KNeighborsRegressor()
>>> knr.n_neighbors=3
>>> knr.fit(train_x, train_y)

>>> for i in [i for i in range(40, 51)]:
>>>     print(f"length={i} ==> pred_weight={knr.predict([[i]])}")
length=40 ==> pred_weight=[921.66666667]
length=41 ==> pred_weight=[921.66666667]
length=42 ==> pred_weight=[1066.66666667]
length=43 ==> pred_weight=[1033.33333333]
length=44 ==> pred_weight=[1033.33333333]
length=45 ==> pred_weight=[1033.33333333]
length=46 ==> pred_weight=[1033.33333333]
length=47 ==> pred_weight=[1033.33333333]
length=48 ==> pred_weight=[1033.33333333]
length=49 ==> pred_weight=[1033.33333333]
length=50 ==> pred_weight=[1033.33333333]
```
결과를 보면 알 수 있는데, 독립변수의 값이 어떤 지점 이상으로 올라가면 추정값이 모두 일관되게 나타난다. 가까운 이웃이 모두 동일해질 것이기 때문이다. (아래 그림을 보면 이해가 쉽다.)

###### length=40
<img src="https://github.com/user-attachments/assets/2e9c56a3-0de7-4c4d-a54e-748a82cc782b" width="30%" />

###### length=42
<img src="https://github.com/user-attachments/assets/c905e2d2-872b-4ebc-831f-31b73c37b16f" width="30%" />

###### length=43
<img src="https://github.com/user-attachments/assets/af5513a1-0886-4a4d-8cb5-317291a5ea68" width="30%" />

###### length=45
<img src="https://github.com/user-attachments/assets/9f331b89-0ea6-49a4-acd5-c76ab19c9f82" width="30%" />

###### length=50
<img src="https://github.com/user-attachments/assets/b97c16c6-6ab4-4919-aaee-95687924b9ff" width="30%" />

정리하자면, KNeighborsRegression은 어떠한 범위 내에서는 추정을 곧잘 하지만, 그 범위를 벗어나면 추정을 제대로 하지 못한다.

#### LinearRegression

여기서 우리가 사용할 수 있는 것이 LinearRegression이다. 회귀분석은 통계학에서도 중요하게 다뤄지는 분석방법 중 하나인데, 아주 간단하게 말해서 데이터를 가장 잘 설명하는 하나의 직선의 방정식을 찾는 것이다. 사용하는 법도 아주 간단한데, 모형을 불러오고 독립변수와 종속변수를 fit() 함수에 넣어주면 끝이다.

```python
from sklearn.linear_model import LinearRegression

lr=LinearRegression()
lr.fit(fish_length.reshape(-1,1), fish_weight)          # 한 가지 중요한 것은 독립변수가 2차원 배열이어야 한다.

lr.predict([[50]])        # array([1241.83860323])
```
위 KNR 모델에서는 길이가 43을 넘는 경우 모두 1033.33..으로 예측했지만 선형회귀 모형은 길이가 50인 경우에도 알맞은 추정값을 반환한다.

이를 그래프로 나타내보면 아래와 같다.

<img src="https://github.com/user-attachments/assets/252a016a-d349-4684-935d-09ed0691b98b" width="30%" />

그래프를 보면 2가지 마음에 걸리는 부분이 있다.
1. 길이의 값이 작은 경우에 대해서 무게의 오차가 커보인다.
2. 길이의 값에 따라 무게의 추정값은 작아지다가 음수가 되어버리기도 한다.

이를 해결하기 위해 회귀모형을 2차식으로 만들어 볼 수 있다.

```python
lr = LinearRegression()

train_poly = np.column_stack((train_x**2,train_x))
lr.fit(train_poly, train_y)
lr.predict([[50**2,50]])
```
그래프로 나타내면 아래와 같다.

<img src="https://github.com/user-attachments/assets/8605784b-79e4-46d3-b2cc-363133f2cf51" width="30%" />

다만, 이 모형도 문제가 있는 것은 ① 길이에 따라 무게가 음수가 되는 부분이 아직 조금 남아있다는 것 ② 그리고 2차 모형의 특성상 길이에 따라 무게의 추정값이 기하급수적으로 크게 추정된다는 것이다.

### 정리

기계학습에 대해 조금은 배웠었는데 KNeighborsRegression가 있다는 것을 처음 알았다. 중요도가 떨어져서 가르쳐주지 않았다고 생각했고, 각자 상황에 맞는 방법이 있다고 생각했다. KNN은 분류에 적합한 알고리즘이고 값의 추정에 대해서는 당연히 회귀모형이 더 좋은 모델이라고 생각했다. 그런데 마지막에 길이에 따라 무게가 음수로 추정되는 문제는 KNR을 사용하면 해결될 수 있는 문제이다. 그렇게 생각해보면 언제나 좋은 모델은 역시 존재하지 않는 것이고, 상황에 맞게 도구를 선택하는 것이 중요한 것 같다. ① 범위 내에서는 KNR-범위 밖에서는 LR 또는 ② 길이가 특정 범위 이하에 대해 KNR-그 이상에서는 LR과 같이..
