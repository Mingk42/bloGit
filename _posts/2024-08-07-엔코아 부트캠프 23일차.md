## spark 입문

오늘 드디어 spark가 시작되었다. 전부터 hadoop이나 spark라는 프로그램에 대해서는 많이 들어왔는데 드디어 직접 다뤄볼 수 있게 되었다.

길게 설명해주셨는데 기억에 남는 것은 hadoop은 디스크 기반으로 read/write가 이뤄져서 속도가 느리다고 한다. 하지만 hadoop의 최고 장점은 장애발생 시에도 안정적으로 데이터를 처리할 수 있다는 것에 있다고 한다. 하지만 여전히 속도에 대한 갈증이 남았고 이를 해결한 것이 spark인 것이다. spark는 인메모리 방식으로 디스크 read 횟수를 줄이고 ram을 활용하여 속도를 개선하였다. 100배 빠르다고 이야기한다고 하는데 실제 그런지는 잘 모르겠다는 말씀도 하셨다 ㅋ_ㅋ

#### zeppelin 설정

먼저 zeppelin과 spark를 설치하고 zeppline에서 다음 명령어를 실행해 보았다. 

```spark
>>> sc.version
```

결과는 에러...

구글링한 결과 특정 파일을 삭제해야 zeppelin이 정상적으로 구동되는 것으로 확인되었다. 

```bash
$ ls -al
total 31M
drwxr-xr-x  8 root2 root2 4.0K Aug  7 15:11 .
drwxr-xr-x 26 root2 root2 4.0K Aug  7 15:11 ..
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._META-INF
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._R
-rw-r--r--  1 root2 root2  163 Aug  7 15:11 ._interpreter-setting.json
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._pyspark
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._python
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._scala-2.12
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._scala-2.13
-rw-r--r--  1 root2 root2  163 Aug  7 15:11 ._spark-interpreter-0.11.1.jar
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 META-INF
drwxr-xr-x  3 root2 root2 4.0K Aug  7 15:11 R
-rw-r--r--  1 root2 root2  12K Aug  7 15:11 interpreter-setting.json
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 pyspark
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 python
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 scala-2.12
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 scala-2.13
-rw-r--r--  1 root2 root2  31M Aug  7 15:11 spark-interpreter-0.11.1.jar
```
위에서 `._`로 시작하는 파일을 모두 지우고, 다시 각 폴더에서 해당 문자열로 시작하는 파일을 모두 지웠더니 정상적으로 결과를 얻을 수 있었다.

```spark
>>> sc.version
res4: String = 3.5.1
```
정상 동작!! 바로 parquet를 읽어본다.

```spark
>>> df1 = spark.read.parquet("/home/root2/tmp/sparkPartition")
>>> df1.show(3)
+----+----+---------+-------------+--------+--------------------+----------+----------+----------+----------+-----------+-----------+-------+---------+----------+-------+-------+-------+--------+------------+-----------+
|rnum|rank|rankInten|rankOldAndNew| movieCd|             movieNm|    openDt|  salesAmt|salesShare|salesInten|salesChange|   salesAcc|audiCnt|audiInten|audiChange|audiAcc|scrnCnt|showCnt| load_dt|multiMovieYn|repNationCd|
+----+----+---------+-------------+--------+--------------------+----------+----------+----------+----------+-----------+-----------+-------+---------+----------+-------+-------+-------+--------+------------+-----------+
|   1|   1|        1|          OLD|20143642|            테이큰 3|2015-01-01|2644551100|      47.4|1640018100|      163.3| 3658460100| 321653|   177118|     122.5| 467280|    614|   2947|20150101|        NULL|          F|
|   2|   2|       -1|          OLD|20149859|   마다가스카의 펭귄|2014-12-31|1687516200|      30.3| 672528400|       66.3| 2736013000| 212779|    67641|      46.6| 361669|    594|   1846|20150101|        NULL|          F|
|   3|   3|        0|          OLD|20140226|호빗: 다섯 군대 전투|2014-12-17| 609678800|      10.9| 167361700|       37.8|22038162144|  69988|     8214|      13.3|2579875|    346|    825|20150101|        NULL|          F|
+----+----+---------+-------------+--------+--------------------+----------+----------+----------+----------+-----------+-----------+-------+---------+----------+-------+-------+-------+--------+------------+-----------+
only showing top 3 rows
```

너무 column이 많은 것 같으니 3개만 남겨보자.

```
>>> df2 = df1["movieCd","multiMovieCd","repNationCd"]
>>> df2.show(3)
+--------+------------+-----------+
| movieCd|multiMovieYn|repNationCd|
+--------+------------+-----------+
|20143642|        NULL|          F|
|20149859|        NULL|          F|
|20140226|        NULL|          F|
+--------+------------+-----------+
only showing top 3 rows
```

movieCd는 key로써 사용될 수 있고 나머지 2개의 column은 우리의 관심대상이다. 이제 각자가 null이 아닌 부분만 남기고 merge해보자.

```
>>> df3 = spark.sql("select * from movie_type where multiMovieYn is null")
>>> df3.createOrReplaceTempView("multi_null")

>>> df4 = spark.sql("select * from movie_type where repNationCd is null")
>>> df4.createOrReplaceTempView("nation_null")

>>> spark.sql("select * from multi_null m join nation_null n on m.movieCd=n.movieCd")
```
여기서 2가지를 살펴보자. 먼저 `spark.sql`을 이용해서 중간에 SQL query를 사용할 수 있다. 그리고 `createOrReplaceTempView`라는 함수를 사용해서 임시 뷰 테이블을 생성할 수 있고, 역시 SQL로 조작할 수 있다.

```
>>> df_join=spark.sql("select * from multi_null m join nation_null n on m.movieCd=n.movieCd")
>>> df_join.show()
+--------+------------+-----------+--------+------------+-----------+
| movieCd|multiMovieYn|repNationCd| movieCd|multiMovieYn|repNationCd|
+--------+------------+-----------+--------+------------+-----------+
|20143642|        NULL|          F|20143642|           N|       NULL|
|20149859|        NULL|          F|20149859|           N|       NULL|
|20140226|        NULL|          F|20140226|           N|       NULL|
|20143344|        NULL|          F|20143344|           Y|       NULL|
|20149120|        NULL|          F|20149120|           N|       NULL|
|20143602|        NULL|          F|20143602|           N|       NULL|
|20143422|        NULL|          F|20143422|           N|       NULL|
|20143045|        NULL|          F|20143045|           Y|       NULL|
|20141621|        NULL|          F|20141621|           Y|       NULL|
|20143343|        NULL|          F|20143343|           Y|       NULL|
|20137048|        NULL|          K|20137048|           N|       NULL|
|20141111|        NULL|          K|20141111|           Y|       NULL|
|20149265|        NULL|          K|20149265|           N|       NULL|
|20130574|        NULL|          K|20130574|           Y|       NULL|
|20147176|        NULL|          K|20147176|           N|       NULL|
|20141614|        NULL|          K|20141614|           Y|       NULL|
|20143639|        NULL|          K|20143639|           N|       NULL|
|20143081|        NULL|          K|20143081|           Y|       NULL|
+--------+------------+-----------+--------+------------+-----------+
```
여기서 한 가지 알 수 있는 건 spark.sql의 결과는 DataFrame이다. 그렇다면 createOrReplaceTempView는 DataFrame을 임시 뷰 테이블로 변환하는 것이라고 이해할 수 있다.

### 정리

오늘은 spark에 대해 배웠다. 처음에 시작부터 오류가 나서 너무 어려웠지만 새로운 것을 많이 배워서 재밌었던 것 같다.
