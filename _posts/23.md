## spark 입문

오늘 드디어 spark가 시작되었다. 전부터 hadoop이나 spark라는 프로그램에 대해서는 많이 들어왔는데 드디어 직접 다뤄볼 수 있게 되었다.

길게 설명해주셨는데 기억에 남는 것은 hadoop은 디스크 기반으로 read/write가 이뤄져서 속도가 느리다고 한다. 하지만 hadoop의 최고 장점은 장애발생 시에도 안정적으로 데이터를 처리할 수 있다는 것에 있다고 한다. 하지만 여전히 속도에 대한 갈증이 남았고 이를 해결한 것이 spark인 것이다. spark는 인메모리 방식으로 디스크 read 횟수를 줄이고 ram을 활용하여 속도를 개선하였다. 100배 빠르다고 이야기한다고 하는데 실제 그런지는 잘 모르겠다는 말씀도 하셨다 ㅋ_ㅋ

#### zeppelin 설정

먼저 zeppelin과 spark를 설치하고 zeppline에서 다음 명령어를 실행해 보았다. 

```spark
spark> sc.version
```

결과는 에러...

구글링한 결과 특정 파일을 삭제해야 zeppelin이 정상적으로 구동되는 것으로 확인되었다. 

```bash
$ ls -al
total 31M
drwxr-xr-x  8 root2 root2 4.0K Aug  7 15:11 .
drwxr-xr-x 26 root2 root2 4.0K Aug  7 15:11 ..
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._META-INF
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._R
-rw-r--r--  1 root2 root2  163 Aug  7 15:11 ._interpreter-setting.json
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._pyspark
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._python
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._scala-2.12
-rwxr-xr-x  1 root2 root2  163 Aug  7 15:11 ._scala-2.13
-rw-r--r--  1 root2 root2  163 Aug  7 15:11 ._spark-interpreter-0.11.1.jar
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 META-INF
drwxr-xr-x  3 root2 root2 4.0K Aug  7 15:11 R
-rw-r--r--  1 root2 root2  12K Aug  7 15:11 interpreter-setting.json
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 pyspark
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 python
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 scala-2.12
drwxr-xr-x  2 root2 root2 4.0K Aug  7 15:11 scala-2.13
-rw-r--r--  1 root2 root2  31M Aug  7 15:11 spark-interpreter-0.11.1.jar
```
위에서 `._`로 시작하는 파일을 모두 지우고, 다시 각 폴더에서 해당 문자열로 시작하는 파일을 모두 지웠더니 정상적으로 결과를 얻을 수 있었다.

```spark
spark> sc.version
res4: String = 3.5.1
```

### 정리
