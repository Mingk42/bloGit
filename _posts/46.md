## KNeighborsRegression, LinearRegression

#### KNeighborsRegression

지난 주의 분류에 이어서 오늘은 회귀모형에 대해 학습하였다. 먼저 KNeighborsRegression으로 회귀모형에 대한 학습을 시작하였다. KNeighborsRegression은 지난 주에 분류모형에 사용했던 KNN과 비슷한 방식으로 동작하는데 근접한 이웃들의 평균울 회귀모형의 추정값으로 반환하는 방식이라고 한다. 그리고 이러한 모형의 한계는 뚜렷하다.

```
>>> knr=KNeighborsRegressor()
>>> knr.n_neighbors=3
>>> knr.fit(train_x, train_y)

>>> for i in [i for i in range(40, 51)]:
>>>     print(f"length={i} ==> pred_weight={knr.predict([[i]])}")
length=40 ==> pred_weight=[921.66666667]
length=41 ==> pred_weight=[921.66666667]
length=42 ==> pred_weight=[1066.66666667]
length=43 ==> pred_weight=[1033.33333333]
length=44 ==> pred_weight=[1033.33333333]
length=45 ==> pred_weight=[1033.33333333]
length=46 ==> pred_weight=[1033.33333333]
length=47 ==> pred_weight=[1033.33333333]
length=48 ==> pred_weight=[1033.33333333]
length=49 ==> pred_weight=[1033.33333333]
length=50 ==> pred_weight=[1033.33333333]
```
결과를 보면 알 수 있는데, 독립변수의 값이 어떤 지점 이상으로 올라가면 추정값이 모두 일관되게 나타난다. 가까운 이웃이 모두 동일해질 것이기 때문이다. (아래 그림을 보면 이해가 쉽다.)

###### length=40
###### length=42
###### length=43
###### length=45
###### length=50

정리하자면, KNeighborsRegression은 어떠한 범위 내에서는 추정을 곧잘 하지만, 그 범위를 벗어나면 추정을 제대로 하지 못한다.

#### LinearRegression

여기서 우리가 사용할 수 있는 것이 LinearRegression이다. 회귀분석은 통계학에서도 중요하게 다뤄지는 분석방법 중 하나인데, 아주 간단하게 말해서 데이터를 가장 잘 설명하는 하나의 직선의 방정식을 찾는 것이다. 사용하는 법도 아주 간단한데, 모형을 불러오고 독립변수와 종속변수를 fit() 함수에 넣어주면 끝이다.

```python
from sklearn.linear_model import LinearRegression

lr=LinearRegression()
lr.fit(fish_length.reshape(-1,1), fish_weight)          # 한 가지 중요한 것은 독립변수가 2차원 배열이어야 한다.
```
이를 그래프로 나타내보면 아래와 같다.



### 정리

 기계학습에 대해 조금은 배웠었는데 KNeighborsRegression가 있다는 것을 처음 알았다.
