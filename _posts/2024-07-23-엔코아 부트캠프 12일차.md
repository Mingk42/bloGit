## parquet 살짝만 알아보기

오늘은 parquet에 대해 소개하면서 수업을 시작했다.

하둡과 같은 빅데이터 처리시스템에서 메모리 사용량과 데이터의 크기를 줄이는 것은 중요한 이슈 중에 하나이다.

전통적으로 데이터는 행 기반으로 압축되었다. 하나의 행이 하나의 개체에 대한 데이터이므로 이런 방식은 자연스럽게 느껴진다.
그런데 행 기반으로 압축을 하는 것보다 열 기반으로 압축을 하는 것이 중복된 데이터가 있을 확률이 크다. (열은 같은 도메인을 가지므로) 이런 아이디어로 나타난 것이 parquet이다. 
parquet을 사용하면 압축률이 높아진다. 이렇게 압축된 데이터는 hash로 변환하고 link를 이용해 데이터를 저장한다. 

또한, parquet을 사용하면 전체 컬럼 중 일부의 컬럼만 선택하여 가져오게 되므로 데이터 I/O를 줄일 수 있다. 그리고 컬럼별로 데이터에 적합한 인코딩을 적용하기에 유리하다는 이점이 있다. 

좀 더 깊은 내용은 차차 알아가기로.. [databricks-parquet](https://www.databricks.com/kr/glossary/what-is-parquet)

#### tmp to base

어제 csv파일을 tmp 테이블로 옮기는 것까지 완료되었다. 이제 tmp 테이블에서 데이터를 적절하게 수정하여 base 테이블에 적재하면 된다.

이전에도 언급된 적 있지만, tmp 테이블을 거쳐 base 테이블에 적재하는 것은 이점이 있다.

멱등성을 유지하는 데 유리하다. 그리고 데이터 파이프라인은 장애 발생시 작고 빠르게 해당 작업을 재시도하는 것이 중요한데, 작업을 작은 단위로 나눔으로써 해당 요구를 만족시킬 수 있다.

```sql
                INSERT INTO cmd_usage
                    SELECT
                       NULLIF(STR_TO_DATE(dt,'%Y-%m-%d'), STR_TO_DATE('1970-01-01','%Y-%m-%d')) dt,
                       command,
                       CAST(cnt as unsigned) cnt
                    FROM tmp_cmd_usage
                    WHERE dt={{ds}}
		;
``` 
최초로 작성했던 퀴리이다. SELECT까지는 정상적으로 동작했는데, INSERT에서는 계속 오류가 났다. (일부러 dt에 도메인에 맞지 않는 형식의 날짜값을 넣었다.) 

그래서 아래와 같이 수정했다.
```sql
                INSERT INTO cmd_usage
	            SELECT
                        CASE WHEN dt LIKE '%-%-%' THEN STR_TO_DATE(dt, '%Y-%m-%d')
                        ELSE STR_TO_DATE('1970-01-01', '%Y-%m-%d')  END dt,
                        command,
                        CASE WHEN cnt REGEXP '[0-9]+$' THEN CAST(cnt AS UNSIGNED)
                        ELSE -1 END cnt
                    FROM tmp_cmd_usage
                    WHERE dt='{{ds}}'
		;
```

`CASE`문으로 처리했다. CASE는 일반적인 프로그래밍 언어에서 IF와 같다고 할 수 있다.

정상동작 완료!


#### 다음 파이프라인 뼈대 만들기

해당 파이프라인에서 해야할 작업은 모두 끝났다. 다음 파이프라인을 만들어 본다.
![image](https://github.com/user-attachments/assets/dd69bb41-9a9f-4da5-9973-07eb30164806)
뼈대는 위와 같다. 지금까지 만든 파이프라인이 정상동작했는지 확인하고 parquet을 만든다.

parqeut에 대해 자세히 알아보기 위해 pandas를 조금 배워보자.

#### pandas

```python
>> import pandas as pd

>> df = pd.read_csv('/home/root2/data/csv/20240717/count.csv', 
                 on_bad_lines='skip', 
                 encoding_errors='ignore',
                 names=['dt', 'cmd', 'cnt']
                )

>> df['dt']=df['dt'].str.replace('^','')
>> df['cmd']=df['cmd'].str.replace('^','')
>> df['cnt']=df['cnt'].str.replace('^','').astype(int)

>> fdf=df[df['cmd'].str.contains('aws')]
>> print(fdf['cnt'].sum())                 # result : 34

>> df.dtypes
dt     object
cmd    object
cnt     int64
dtype: object

>> df.to_parquet('~/tmp.history.parquet')
>> df2=pd.read_parquet('~/tmp/history.parquet')
```
먼저 csv파일을 읽어서 dataframe 형태로 불러왔다. 하나의 엑셀파일이라고 이해하면 조금 이해하기 쉽다. csv파일을 만들 때 여차저차의 이유로 `^`문자를 넣었으므로 제거해주고, cnt컬럼은 int로 형변환을 해줬다. DB에서 LOAD DATA할 때와 달리 무엇을 기준으로 컬럼을 분리할지 지정해주지 않았는데, csv파일이라는 것 자체가 comma seperated value이기 때문에 기본적으로 `,`를 기준으로 분리하게 된다. 그리고 나서 특정문자열을 포함하는 row만 필터링하여 해당 row들의 cnt값을 더하여 출력해주었다.

그리고 dataframe을 parquet으로 저장하고 다시 불러와보았다.

### 정리
`CASE WHEN`구문을 만나서 반가웠다. 회사에 다닐 때 날짜만 변경하는 방법으로 상당히 빈번하게 사용했던 것 같다. 

그리고 pandas에서 `read_csv`는 상당히 자주 사용했었는데, `read_parquet`, `to_parquet`은 처음 사용해봐서 새로웠다.
